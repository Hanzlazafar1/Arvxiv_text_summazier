{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQopCt6wHaOZ0RgmR00rlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanzlazafar1/Arvxiv_text_summazier/blob/main/arvxiv_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PFExCFVqSsn",
        "outputId": "18d5b2da-4107-4111-bb4d-896e9cbb26b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjZJwY9Lqkpk",
        "outputId": "4d2c00f8-6954-4d7d-cf72-0e11d23e483a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=32ce067a2abcfbca87d7d24cdff6889f5279cc354a5ca0fa427b67767759a2ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F9RFBuyxxO0y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI-related papers\n",
        "query = 'ai OR artificial intelligence OR machine learning'\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "# Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "      'published': result.published,\n",
        "        'title': result.title,\n",
        "        'abstract': result.summary,\n",
        "        'categories': result.categories\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wtPJQPUGtOGf",
        "outputId": "9fe7f994-36af-4948-9ae9-6dd5d737ad3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-5f6bc63cd8db>:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2024-11-08 18:50:51+00:00   \n",
              "1 2024-11-08 18:50:37+00:00   \n",
              "2 2024-11-08 18:48:57+00:00   \n",
              "3 2024-11-08 18:47:08+00:00   \n",
              "4 2024-11-08 18:46:40+00:00   \n",
              "5 2024-11-08 18:45:06+00:00   \n",
              "6 2024-11-08 18:43:15+00:00   \n",
              "7 2024-11-08 18:36:33+00:00   \n",
              "8 2024-11-08 18:33:03+00:00   \n",
              "9 2024-11-08 18:26:17+00:00   \n",
              "\n",
              "                                                                                         title  \\\n",
              "0        Safe Reinforcement Learning of Robot Trajectories in the Presence of Moving Obstacles   \n",
              "1                          ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles   \n",
              "2                         Using Language Models to Disambiguate Lexical Choices in Translation   \n",
              "3                                              GazeSearch: Radiology Findings Search Benchmark   \n",
              "4      Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway Tree Segmentation   \n",
              "5                       LLMs as Method Actors: A Model for Prompt Engineering and Architecture   \n",
              "6                  Quantitative Assessment of Intersectional Empathetic Bias and Understanding   \n",
              "7                       Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?   \n",
              "8  Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems   \n",
              "9     FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            abstract  \\\n",
              "0                                                                                                                                                                             In this paper, we present an approach for learning collision-free robot\\ntrajectories in the presence of moving obstacles. As a first step, we train a\\nbackup policy to generate evasive movements from arbitrary initial robot states\\nusing model-free reinforcement learning. When learning policies for other\\ntasks, the backup policy can be used to estimate the potential risk of a\\ncollision and to offer an alternative action if the estimated risk is\\nconsidered too high. No matter which action is selected, our action space\\nensures that the kinematic limits of the robot joints are not violated. We\\nanalyze and evaluate two different methods for estimating the risk of a\\ncollision. A physics simulation performed in the background is computationally\\nexpensive but provides the best results in deterministic environments. If a\\ndata-based risk estimator is used instead, the computational effort is\\nsignificantly reduced, but an additional source of error is introduced. For\\nevaluation, we successfully learn a reaching task and a basketball task while\\nkeeping the risk of collisions low. The results demonstrate the effectiveness\\nof our approach for deterministic and stochastic environments, including a\\nhuman-robot scenario and a ball environment, where no state can be considered\\npermanently safe. By conducting experiments with a real robot, we show that our\\napproach can generate safe trajectories in real time.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Deaf and hard-of-hearing (DHH) students face significant barriers in\\naccessing science, technology, engineering, and mathematics (STEM) education,\\nnotably due to the scarcity of STEM resources in signed languages. To help\\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\\narticles on STEM topics in English, interpreted into over 300 hours of American\\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\\nfocused on STEM, facilitating the development of AI resources for STEM\\neducation in ASL. We identify several use cases of ASL STEM Wiki with\\nhuman-centered applications. For example, because this dataset highlights the\\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\\nstudents' ability to learn, we develop models to identify fingerspelled words\\n-- which can later be used to query for appropriate ASL signs to suggest to\\ninterpreters.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In translation, a concept represented by a single word in a source language\\ncan have multiple variations in a target language. The task of lexical\\nselection requires using context to identify which variation is most\\nappropriate for a source text. We work with native speakers of nine languages\\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\\nconcept variation when translating from English. We evaluate recent LLMs and\\nneural machine translation systems on DTAiLS, with the best-performing model,\\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\\nlanguage models to generate English rules describing target-language concept\\nvariations. Providing weaker models with high-quality lexical rules improves\\naccuracy substantially, in some cases reaching or outperforming GPT-4.   \n",
              "3                                                                                                                                                                                                                                                   Medical eye-tracking data is an important information source for\\nunderstanding how radiologists visually interpret medical images. This\\ninformation not only improves the accuracy of deep learning models for X-ray\\nanalysis but also their interpretability, enhancing transparency in\\ndecision-making. However, the current eye-tracking data is dispersed,\\nunprocessed, and ambiguous, making it difficult to derive meaningful insights.\\nTherefore, there is a need to create a new dataset with more focus and\\npurposeful eyetracking data, improving its utility for diagnostic applications.\\nIn this work, we propose a refinement method inspired by the target-present\\nvisual search challenge: there is a specific finding and fixations are guided\\nto locate it. After refining the existing eye-tracking datasets, we transform\\nthem into a curated visual search dataset, called GazeSearch, specifically for\\nradiology findings, where each fixation sequence is purposefully aligned to the\\ntask of locating a particular finding. Subsequently, we introduce a scan path\\nprediction baseline, called ChestSearch, specifically tailored to GazeSearch.\\nFinally, we employ the newly introduced GazeSearch as a benchmark to evaluate\\nthe performance of current state-of-the-art methods, offering a comprehensive\\nassessment for visual search in the medical imaging domain.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Despite advances with deep learning (DL), automated airway segmentation from\\nchest CT scans continues to face challenges in segmentation quality and\\ngeneralization across cohorts. To address these, we propose integrating\\nCurriculum Learning (CL) into airway segmentation networks, distributing the\\ntraining set into batches according to ad-hoc complexity scores derived from CT\\nscans and corresponding ground-truth tree features. We specifically investigate\\nfew-shot domain adaptation, targeting scenarios where manual annotation of a\\nfull fine-tuning dataset is prohibitively expensive. Results are reported on\\ntwo large open-cohorts (ATM22 and AIIB23) with high performance using CL for\\nfull training (Source domain) and few-shot fine-tuning (Target domain), but\\nwith also some insights on potential detrimental effects if using a classic\\nBootstrapping scoring function or if not using proper scan sequencing.   \n",
              "5                                                                                                                                                                                                                                                                                                                                We introduce \"Method Actors\" as a mental model for guiding LLM prompt\\nengineering and prompt architecture. Under this mental model, LLMs should be\\nthought of as actors; prompts as scripts and cues; and LLM responses as\\nperformances. We apply this mental model to the task of improving LLM\\nperformance at playing Connections, a New York Times word puzzle game that\\nprior research identified as a challenging benchmark for evaluating LLM\\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\\nsignificantly improve LLM performance over both a vanilla and \"Chain of\\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\\nnewest model designed specifically for complex reasoning tasks, o1-preview.\\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\\na time over multiple API calls, o1-preview solves 100% of the puzzles.\\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\\npuzzles that o1-preview solves perfectly from 76% to 87%.   \n",
              "6  A growing amount of literature critiques the current operationalizations of\\nempathy based on loose definitions of the construct. Such definitions\\nnegatively affect dataset quality, model robustness, and evaluation\\nreliability. We propose an empathy evaluation framework that operationalizes\\nempathy close to its psychological origins. The framework measures the variance\\nin responses of LLMs to prompts using existing metrics for empathy and\\nemotional valence. The variance is introduced through the controlled generation\\nof the prompts by varying social biases affecting context understanding, thus\\nimpacting empathetic understanding. The control over generation ensures high\\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\\nhigh-quality translation, especially into languages that currently have\\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\\nempathy evaluation with the framework, including multiple-choice answers and\\nfree generation. The variance in our initial evaluation sample is small and we\\nwere unable to measure convincing differences between the empathetic\\nunderstanding in contexts given by different social groups. However, the\\nresults are promising because the models showed significant alterations their\\nreasoning chains needed to capture the relatively subtle changes in the\\nprompts. This provides the basis for future research into the construction of\\nthe evaluation sample and statistical methods for measuring the results.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Political misinformation poses significant challenges to democratic\\nprocesses, shaping public opinion and trust in media. Manual fact-checking\\nmethods face issues of scalability and annotator bias, while machine learning\\nmodels require large, costly labelled datasets. This study investigates the use\\nof state-of-the-art large language models (LLMs) as reliable annotators for\\ndetecting political factuality in news articles. Using open-source LLMs, we\\ncreate a politically diverse dataset, labelled for bias through LLM-generated\\nannotations. These annotations are validated by human experts and further\\nevaluated by LLM-based judges to assess the accuracy and reliability of the\\nannotations. Our approach offers a scalable and robust alternative to\\ntraditional fact-checking, enhancing transparency and public trust in media.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Equivariant Imaging (EI) regularization has become the de-facto technique for\\nunsupervised training of deep imaging networks, without any need of\\nground-truth data. Observing that the EI-based unsupervised training paradigm\\ncurrently has significant computational redundancy leading to inefficiency in\\nhigh-dimensional applications, we propose a sketched EI regularization which\\nleverages the randomized sketching techniques for acceleration. We then extend\\nour sketched EI regularization to develop an accelerated deep internal learning\\nframework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be\\nefficiently applied for single-image and task-adapted reconstruction. Our\\nnumerical study on X-ray CT image reconstruction tasks demonstrate that our\\napproach can achieve order-of-magnitude computational acceleration over\\nstandard EI-based counterpart in single-input setting, and network adaptation\\nat test time.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We introduce FinDVer, a comprehensive benchmark specifically designed to\\nevaluate the explainable claim verification capabilities of LLMs in the context\\nof understanding and analyzing long, hybrid-content financial documents.\\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\\neach addressing common scenarios encountered in real-world financial contexts.\\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\\nresults show that even the current best-performing system, GPT-4o, still lags\\nbehind human experts. We further provide in-depth analysis on long-context and\\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\\ninsights to drive future advancements. We believe that FinDVer can serve as a\\nvaluable benchmark for evaluating LLMs in claim verification over complex,\\nexpert-domain documents.   \n",
              "\n",
              "                            categories  \n",
              "0                              [cs.RO]  \n",
              "1         [cs.CL, cs.AI, cs.CV, cs.HC]  \n",
              "2                       [cs.CL, cs.AI]  \n",
              "3                       [cs.CV, cs.AI]  \n",
              "4                       [cs.CV, cs.LG]  \n",
              "5                       [cs.AI, cs.CL]  \n",
              "6  [cs.CL, cs.AI, cs.HC, 68T50, I.2.7]  \n",
              "7                       [cs.CL, cs.AI]  \n",
              "8     [eess.IV, cs.CV, cs.LG, math.OC]  \n",
              "9                       [cs.CL, cs.LG]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acc20014-5589-4826-b746-ccd3d4ad07c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-11-08 18:50:51+00:00</td>\n",
              "      <td>Safe Reinforcement Learning of Robot Trajectories in the Presence of Moving Obstacles</td>\n",
              "      <td>In this paper, we present an approach for learning collision-free robot\\ntrajectories in the presence of moving obstacles. As a first step, we train a\\nbackup policy to generate evasive movements from arbitrary initial robot states\\nusing model-free reinforcement learning. When learning policies for other\\ntasks, the backup policy can be used to estimate the potential risk of a\\ncollision and to offer an alternative action if the estimated risk is\\nconsidered too high. No matter which action is selected, our action space\\nensures that the kinematic limits of the robot joints are not violated. We\\nanalyze and evaluate two different methods for estimating the risk of a\\ncollision. A physics simulation performed in the background is computationally\\nexpensive but provides the best results in deterministic environments. If a\\ndata-based risk estimator is used instead, the computational effort is\\nsignificantly reduced, but an additional source of error is introduced. For\\nevaluation, we successfully learn a reaching task and a basketball task while\\nkeeping the risk of collisions low. The results demonstrate the effectiveness\\nof our approach for deterministic and stochastic environments, including a\\nhuman-robot scenario and a ball environment, where no state can be considered\\npermanently safe. By conducting experiments with a real robot, we show that our\\napproach can generate safe trajectories in real time.</td>\n",
              "      <td>[cs.RO]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-11-08 18:50:37+00:00</td>\n",
              "      <td>ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles</td>\n",
              "      <td>Deaf and hard-of-hearing (DHH) students face significant barriers in\\naccessing science, technology, engineering, and mathematics (STEM) education,\\nnotably due to the scarcity of STEM resources in signed languages. To help\\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\\narticles on STEM topics in English, interpreted into over 300 hours of American\\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\\nfocused on STEM, facilitating the development of AI resources for STEM\\neducation in ASL. We identify several use cases of ASL STEM Wiki with\\nhuman-centered applications. For example, because this dataset highlights the\\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\\nstudents' ability to learn, we develop models to identify fingerspelled words\\n-- which can later be used to query for appropriate ASL signs to suggest to\\ninterpreters.</td>\n",
              "      <td>[cs.CL, cs.AI, cs.CV, cs.HC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-11-08 18:48:57+00:00</td>\n",
              "      <td>Using Language Models to Disambiguate Lexical Choices in Translation</td>\n",
              "      <td>In translation, a concept represented by a single word in a source language\\ncan have multiple variations in a target language. The task of lexical\\nselection requires using context to identify which variation is most\\nappropriate for a source text. We work with native speakers of nine languages\\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\\nconcept variation when translating from English. We evaluate recent LLMs and\\nneural machine translation systems on DTAiLS, with the best-performing model,\\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\\nlanguage models to generate English rules describing target-language concept\\nvariations. Providing weaker models with high-quality lexical rules improves\\naccuracy substantially, in some cases reaching or outperforming GPT-4.</td>\n",
              "      <td>[cs.CL, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-11-08 18:47:08+00:00</td>\n",
              "      <td>GazeSearch: Radiology Findings Search Benchmark</td>\n",
              "      <td>Medical eye-tracking data is an important information source for\\nunderstanding how radiologists visually interpret medical images. This\\ninformation not only improves the accuracy of deep learning models for X-ray\\nanalysis but also their interpretability, enhancing transparency in\\ndecision-making. However, the current eye-tracking data is dispersed,\\nunprocessed, and ambiguous, making it difficult to derive meaningful insights.\\nTherefore, there is a need to create a new dataset with more focus and\\npurposeful eyetracking data, improving its utility for diagnostic applications.\\nIn this work, we propose a refinement method inspired by the target-present\\nvisual search challenge: there is a specific finding and fixations are guided\\nto locate it. After refining the existing eye-tracking datasets, we transform\\nthem into a curated visual search dataset, called GazeSearch, specifically for\\nradiology findings, where each fixation sequence is purposefully aligned to the\\ntask of locating a particular finding. Subsequently, we introduce a scan path\\nprediction baseline, called ChestSearch, specifically tailored to GazeSearch.\\nFinally, we employ the newly introduced GazeSearch as a benchmark to evaluate\\nthe performance of current state-of-the-art methods, offering a comprehensive\\nassessment for visual search in the medical imaging domain.</td>\n",
              "      <td>[cs.CV, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-11-08 18:46:40+00:00</td>\n",
              "      <td>Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway Tree Segmentation</td>\n",
              "      <td>Despite advances with deep learning (DL), automated airway segmentation from\\nchest CT scans continues to face challenges in segmentation quality and\\ngeneralization across cohorts. To address these, we propose integrating\\nCurriculum Learning (CL) into airway segmentation networks, distributing the\\ntraining set into batches according to ad-hoc complexity scores derived from CT\\nscans and corresponding ground-truth tree features. We specifically investigate\\nfew-shot domain adaptation, targeting scenarios where manual annotation of a\\nfull fine-tuning dataset is prohibitively expensive. Results are reported on\\ntwo large open-cohorts (ATM22 and AIIB23) with high performance using CL for\\nfull training (Source domain) and few-shot fine-tuning (Target domain), but\\nwith also some insights on potential detrimental effects if using a classic\\nBootstrapping scoring function or if not using proper scan sequencing.</td>\n",
              "      <td>[cs.CV, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-11-08 18:45:06+00:00</td>\n",
              "      <td>LLMs as Method Actors: A Model for Prompt Engineering and Architecture</td>\n",
              "      <td>We introduce \"Method Actors\" as a mental model for guiding LLM prompt\\nengineering and prompt architecture. Under this mental model, LLMs should be\\nthought of as actors; prompts as scripts and cues; and LLM responses as\\nperformances. We apply this mental model to the task of improving LLM\\nperformance at playing Connections, a New York Times word puzzle game that\\nprior research identified as a challenging benchmark for evaluating LLM\\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\\nsignificantly improve LLM performance over both a vanilla and \"Chain of\\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\\nnewest model designed specifically for complex reasoning tasks, o1-preview.\\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\\na time over multiple API calls, o1-preview solves 100% of the puzzles.\\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\\npuzzles that o1-preview solves perfectly from 76% to 87%.</td>\n",
              "      <td>[cs.AI, cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2024-11-08 18:43:15+00:00</td>\n",
              "      <td>Quantitative Assessment of Intersectional Empathetic Bias and Understanding</td>\n",
              "      <td>A growing amount of literature critiques the current operationalizations of\\nempathy based on loose definitions of the construct. Such definitions\\nnegatively affect dataset quality, model robustness, and evaluation\\nreliability. We propose an empathy evaluation framework that operationalizes\\nempathy close to its psychological origins. The framework measures the variance\\nin responses of LLMs to prompts using existing metrics for empathy and\\nemotional valence. The variance is introduced through the controlled generation\\nof the prompts by varying social biases affecting context understanding, thus\\nimpacting empathetic understanding. The control over generation ensures high\\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\\nhigh-quality translation, especially into languages that currently have\\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\\nempathy evaluation with the framework, including multiple-choice answers and\\nfree generation. The variance in our initial evaluation sample is small and we\\nwere unable to measure convincing differences between the empathetic\\nunderstanding in contexts given by different social groups. However, the\\nresults are promising because the models showed significant alterations their\\nreasoning chains needed to capture the relatively subtle changes in the\\nprompts. This provides the basis for future research into the construction of\\nthe evaluation sample and statistical methods for measuring the results.</td>\n",
              "      <td>[cs.CL, cs.AI, cs.HC, 68T50, I.2.7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2024-11-08 18:36:33+00:00</td>\n",
              "      <td>Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?</td>\n",
              "      <td>Political misinformation poses significant challenges to democratic\\nprocesses, shaping public opinion and trust in media. Manual fact-checking\\nmethods face issues of scalability and annotator bias, while machine learning\\nmodels require large, costly labelled datasets. This study investigates the use\\nof state-of-the-art large language models (LLMs) as reliable annotators for\\ndetecting political factuality in news articles. Using open-source LLMs, we\\ncreate a politically diverse dataset, labelled for bias through LLM-generated\\nannotations. These annotations are validated by human experts and further\\nevaluated by LLM-based judges to assess the accuracy and reliability of the\\nannotations. Our approach offers a scalable and robust alternative to\\ntraditional fact-checking, enhancing transparency and public trust in media.</td>\n",
              "      <td>[cs.CL, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2024-11-08 18:33:03+00:00</td>\n",
              "      <td>Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems</td>\n",
              "      <td>Equivariant Imaging (EI) regularization has become the de-facto technique for\\nunsupervised training of deep imaging networks, without any need of\\nground-truth data. Observing that the EI-based unsupervised training paradigm\\ncurrently has significant computational redundancy leading to inefficiency in\\nhigh-dimensional applications, we propose a sketched EI regularization which\\nleverages the randomized sketching techniques for acceleration. We then extend\\nour sketched EI regularization to develop an accelerated deep internal learning\\nframework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be\\nefficiently applied for single-image and task-adapted reconstruction. Our\\nnumerical study on X-ray CT image reconstruction tasks demonstrate that our\\napproach can achieve order-of-magnitude computational acceleration over\\nstandard EI-based counterpart in single-input setting, and network adaptation\\nat test time.</td>\n",
              "      <td>[eess.IV, cs.CV, cs.LG, math.OC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2024-11-08 18:26:17+00:00</td>\n",
              "      <td>FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents</td>\n",
              "      <td>We introduce FinDVer, a comprehensive benchmark specifically designed to\\nevaluate the explainable claim verification capabilities of LLMs in the context\\nof understanding and analyzing long, hybrid-content financial documents.\\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\\neach addressing common scenarios encountered in real-world financial contexts.\\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\\nresults show that even the current best-performing system, GPT-4o, still lags\\nbehind human experts. We further provide in-depth analysis on long-context and\\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\\ninsights to drive future advancements. We believe that FinDVer can serve as a\\nvaluable benchmark for evaluating LLMs in claim verification over complex,\\nexpert-domain documents.</td>\n",
              "      <td>[cs.CL, cs.LG]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acc20014-5589-4826-b746-ccd3d4ad07c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-acc20014-5589-4826-b746-ccd3d4ad07c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-acc20014-5589-4826-b746-ccd3d4ad07c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9baded06-63f0-4227-9604-d8d2ec06cd18\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9baded06-63f0-4227-9604-d8d2ec06cd18')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9baded06-63f0-4227-9604-d8d2ec06cd18 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-08 18:26:17+00:00\",\n        \"max\": \"2024-11-08 18:50:51+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-11-08 18:33:03+00:00\",\n          \"2024-11-08 18:50:37+00:00\",\n          \"2024-11-08 18:45:06+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems\",\n          \"ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles\",\n          \"LLMs as Method Actors: A Model for Prompt Engineering and Architecture\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Equivariant Imaging (EI) regularization has become the de-facto technique for\\nunsupervised training of deep imaging networks, without any need of\\nground-truth data. Observing that the EI-based unsupervised training paradigm\\ncurrently has significant computational redundancy leading to inefficiency in\\nhigh-dimensional applications, we propose a sketched EI regularization which\\nleverages the randomized sketching techniques for acceleration. We then extend\\nour sketched EI regularization to develop an accelerated deep internal learning\\nframework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be\\nefficiently applied for single-image and task-adapted reconstruction. Our\\nnumerical study on X-ray CT image reconstruction tasks demonstrate that our\\napproach can achieve order-of-magnitude computational acceleration over\\nstandard EI-based counterpart in single-input setting, and network adaptation\\nat test time.\",\n          \"Deaf and hard-of-hearing (DHH) students face significant barriers in\\naccessing science, technology, engineering, and mathematics (STEM) education,\\nnotably due to the scarcity of STEM resources in signed languages. To help\\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\\narticles on STEM topics in English, interpreted into over 300 hours of American\\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\\nfocused on STEM, facilitating the development of AI resources for STEM\\neducation in ASL. We identify several use cases of ASL STEM Wiki with\\nhuman-centered applications. For example, because this dataset highlights the\\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\\nstudents' ability to learn, we develop models to identify fingerspelled words\\n-- which can later be used to query for appropriate ASL signs to suggest to\\ninterpreters.\",\n          \"We introduce \\\"Method Actors\\\" as a mental model for guiding LLM prompt\\nengineering and prompt architecture. Under this mental model, LLMs should be\\nthought of as actors; prompts as scripts and cues; and LLM responses as\\nperformances. We apply this mental model to the task of improving LLM\\nperformance at playing Connections, a New York Times word puzzle game that\\nprior research identified as a challenging benchmark for evaluating LLM\\nreasoning. Our experiments with GPT-4o show that a \\\"Method Actors\\\" approach can\\nsignificantly improve LLM performance over both a vanilla and \\\"Chain of\\nThoughts\\\" approach. A vanilla approach solves 27% of Connections puzzles in our\\ndataset and a \\\"Chain of Thoughts\\\" approach solves 41% of puzzles, whereas our\\nstrongest \\\"Method Actor\\\" approach solves 86% of puzzles. We also test OpenAI's\\nnewest model designed specifically for complex reasoning tasks, o1-preview.\\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\\na time over multiple API calls, o1-preview solves 100% of the puzzles.\\nIncorporating a \\\"Method Actor\\\" prompt architecture increases the percentage of\\npuzzles that o1-preview solves perfectly from 76% to 87%.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example abstract from API\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") #we can dowanload this model it is 1.63 gb and it will sumarize the data\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "id": "7xfTOgdK46Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0]['summary_text']\n",
        "# it will show the summarization"
      ],
      "metadata": {
        "id": "30OFk_Cs5v9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Q8Q5KO_5t-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}